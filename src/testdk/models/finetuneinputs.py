"""Code generated by Speakeasy (https://speakeasy.com). DO NOT EDIT."""

from __future__ import annotations
from enum import Enum
from pydantic import model_serializer
from testdk.types import BaseModel, Nullable, OptionalNullable, UNSET, UNSET_SENTINEL
from typing import Optional
from typing_extensions import NotRequired, TypedDict


class Mode(str, Enum):
    r"""Mode for the fine-tuned model. Allowed values are 'general', 'character', 'style', 'product'. This will affect the caption behaviour. General will describe the image in full detail."""

    GENERAL = "general"
    CHARACTER = "character"
    STYLE = "style"
    PRODUCT = "product"


class Priority(str, Enum):
    r"""Priority of the fine-tuning process. 'speed' will prioritize iteration speed over quality, 'quality' will prioritize quality over speed."""

    SPEED = "speed"
    QUALITY = "quality"
    HIGH_RES_ONLY = "high_res_only"


class FinetuneType(str, Enum):
    r"""Type of fine-tuning. 'lora' is a standard LoRA Adapter, 'full' is a full fine-tuning mode, with a post hoc lora extraction."""

    LORA = "lora"
    FULL = "full"


class LoraRank(int, Enum):
    r"""Rank of the fine-tuned model. 16 or 32. If finetune_type is 'full', this will be the rank of the extracted lora model."""

    SIXTEEN = 16
    THIRTY_TWO = 32


class FinetuneInputsTypedDict(TypedDict):
    file_data: str
    r"""Base64-encoded ZIP file containing training images and, optionally, corresponding captions."""
    finetune_comment: str
    r"""Comment or name of the fine-tuned model. This will be added as a field to the finetune_details."""
    mode: Mode
    r"""Mode for the fine-tuned model. Allowed values are 'general', 'character', 'style', 'product'. This will affect the caption behaviour. General will describe the image in full detail."""
    trigger_word: NotRequired[str]
    r"""Trigger word for the fine-tuned model."""
    iterations: NotRequired[int]
    r"""Number of iterations for fine-tuning."""
    learning_rate: NotRequired[Nullable[float]]
    r"""Learning rate for fine-tuning. If not provided, defaults to 1e-5 for full fine-tuning and 1e-4 for lora fine-tuning."""
    captioning: NotRequired[bool]
    r"""Whether to enable captioning during fine-tuning."""
    priority: NotRequired[Priority]
    r"""Priority of the fine-tuning process. 'speed' will prioritize iteration speed over quality, 'quality' will prioritize quality over speed."""
    finetune_type: NotRequired[FinetuneType]
    r"""Type of fine-tuning. 'lora' is a standard LoRA Adapter, 'full' is a full fine-tuning mode, with a post hoc lora extraction."""
    lora_rank: NotRequired[LoraRank]
    r"""Rank of the fine-tuned model. 16 or 32. If finetune_type is 'full', this will be the rank of the extracted lora model."""
    webhook_url: NotRequired[Nullable[str]]
    r"""URL to receive webhook notifications"""
    webhook_secret: NotRequired[Nullable[str]]
    r"""Optional secret for webhook signature verification"""


class FinetuneInputs(BaseModel):
    file_data: str
    r"""Base64-encoded ZIP file containing training images and, optionally, corresponding captions."""

    finetune_comment: str
    r"""Comment or name of the fine-tuned model. This will be added as a field to the finetune_details."""

    mode: Mode
    r"""Mode for the fine-tuned model. Allowed values are 'general', 'character', 'style', 'product'. This will affect the caption behaviour. General will describe the image in full detail."""

    trigger_word: Optional[str] = "TOK"
    r"""Trigger word for the fine-tuned model."""

    iterations: Optional[int] = 300
    r"""Number of iterations for fine-tuning."""

    learning_rate: OptionalNullable[float] = UNSET
    r"""Learning rate for fine-tuning. If not provided, defaults to 1e-5 for full fine-tuning and 1e-4 for lora fine-tuning."""

    captioning: Optional[bool] = True
    r"""Whether to enable captioning during fine-tuning."""

    priority: Optional[Priority] = Priority.QUALITY
    r"""Priority of the fine-tuning process. 'speed' will prioritize iteration speed over quality, 'quality' will prioritize quality over speed."""

    finetune_type: Optional[FinetuneType] = FinetuneType.FULL
    r"""Type of fine-tuning. 'lora' is a standard LoRA Adapter, 'full' is a full fine-tuning mode, with a post hoc lora extraction."""

    lora_rank: Optional[LoraRank] = LoraRank.THIRTY_TWO
    r"""Rank of the fine-tuned model. 16 or 32. If finetune_type is 'full', this will be the rank of the extracted lora model."""

    webhook_url: OptionalNullable[str] = UNSET
    r"""URL to receive webhook notifications"""

    webhook_secret: OptionalNullable[str] = UNSET
    r"""Optional secret for webhook signature verification"""

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = [
            "trigger_word",
            "iterations",
            "learning_rate",
            "captioning",
            "priority",
            "finetune_type",
            "lora_rank",
            "webhook_url",
            "webhook_secret",
        ]
        nullable_fields = ["learning_rate", "webhook_url", "webhook_secret"]
        null_default_fields = []

        serialized = handler(self)

        m = {}

        for n, f in type(self).model_fields.items():
            k = f.alias or n
            val = serialized.get(k)
            serialized.pop(k, None)

            optional_nullable = k in optional_fields and k in nullable_fields
            is_set = (
                self.__pydantic_fields_set__.intersection({n})
                or k in null_default_fields
            )  # pylint: disable=no-member

            if val is not None and val != UNSET_SENTINEL:
                m[k] = val
            elif val != UNSET_SENTINEL and (
                not k in optional_fields or (optional_nullable and is_set)
            ):
                m[k] = val

        return m
